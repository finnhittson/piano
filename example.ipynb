{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import dataloader\n",
    "import omr\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = \"C:\\\\Users\\\\hitts\\\\Documents\\\\GitHub\\\\piano\"\n",
    "PATH = \"C:\\\\Users\\\\hitts\\\\Desktop\"\n",
    "BATCH_SIZE = 0\n",
    "VAL_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dataloader.DataLoader(path=PATH, batch_size=BATCH_SIZE, val_size=VAL_SIZE, shuffle=False, verbose=True)\n",
    "x_train, y_train, x_test, y_test = dl.get_ds()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = np.random.randint(len(x_train))\n",
    "image = x_train[rand_idx]\n",
    "label = y_train[rand_idx]\n",
    "plt.imshow(image, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabetic_label = dl.decode_numerical_label(label)\n",
    "print(alphabetic_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = omr.get_model(len(dl.get_alphabet()))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=2, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC Loss Function Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_labels = 6\n",
    "max_label_length = 5\n",
    "num_frames = 12\n",
    "labels = tf.random.uniform([batch_size, max_label_length], minval=1, maxval=num_labels, dtype=tf.int32)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.random.uniform([num_frames, batch_size, num_labels])\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_length = tf.random.uniform([batch_size], minval=2, maxval=max_label_length, dtype=tf.int32)\n",
    "label_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mask = tf.sequence_mask(label_length, maxlen=max_label_length, dtype = label_length.dtype)\n",
    "label_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels *= label_mask\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_length = [num_frames] * batch_size\n",
    "logit_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    t.watch(logits)\n",
    "    ref_loss = tf.nn.ctc_loss(\n",
    "        labels = labels,\n",
    "        logits = logits,\n",
    "        label_length = label_length,\n",
    "        logit_length = logit_length,\n",
    "        blank_index = 0\n",
    "    )\n",
    "    print(ref_loss)\n",
    "    print(tf.reduce_mean(ref_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC Loss Function Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\hitts\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\hitts\\AppData\\Local\\Temp\\ipykernel_1088\\4060988590.py\", line 100, in accuracy  *\n        y_true_padded = tf.pad(y_true_split, paddings=[[0, max_length - tf.shape(y_true_split)[0]]])\n\n    TypeError: Failed to convert elements of tf.RaggedTensor(values=Tensor(\"StringsByteSplit/StringSplit:1\", shape=(None,), dtype=string), row_splits=Tensor(\"StringsByteSplit/RaggedFromValueRowIds/RowPartitionFromValueRowIds/concat:0\", shape=(None,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 121\u001b[0m\n\u001b[0;32m    118\u001b[0m test_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39mctc_loss, metrics\u001b[38;5;241m=\u001b[39m[accuracy])\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m \u001b[43mtest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileyp5mlyh_.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filehfv13a40.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__accuracy\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     13\u001b[0m y_pred_split \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mstrings\u001b[38;5;241m.\u001b[39mbytes_split, (ag__\u001b[38;5;241m.\u001b[39mld(y_pred_str),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     14\u001b[0m max_length \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mmaximum, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mshape, (ag__\u001b[38;5;241m.\u001b[39mld(y_true_split),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[\u001b[38;5;241m0\u001b[39m], ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mshape, (ag__\u001b[38;5;241m.\u001b[39mld(y_pred_split),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 15\u001b[0m y_true_padded \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true_split\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpaddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true_split\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m y_pred_padded \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mpad, (ag__\u001b[38;5;241m.\u001b[39mld(y_pred_split),), \u001b[38;5;28mdict\u001b[39m(paddings\u001b[38;5;241m=\u001b[39m[[\u001b[38;5;241m0\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(max_length) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mshape, (ag__\u001b[38;5;241m.\u001b[39mld(y_pred_split),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[\u001b[38;5;241m0\u001b[39m]]]), fscope)\n\u001b[0;32m     17\u001b[0m y_true_sparse \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mfrom_dense, (ag__\u001b[38;5;241m.\u001b[39mld(y_true_padded),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\hitts\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\hitts\\AppData\\Local\\Temp\\ipykernel_1088\\4060988590.py\", line 100, in accuracy  *\n        y_true_padded = tf.pad(y_true_split, paddings=[[0, max_length - tf.shape(y_true_split)[0]]])\n\n    TypeError: Failed to convert elements of tf.RaggedTensor(values=Tensor(\"StringsByteSplit/StringSplit:1\", shape=(None,), dtype=string), row_splits=Tensor(\"StringsByteSplit/RaggedFromValueRowIds/RowPartitionFromValueRowIds/concat:0\", shape=(None,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data (replace this with your data)\n",
    "num_samples = 1000\n",
    "img_height = 28\n",
    "img_width = 28\n",
    "num_classes = 10\n",
    "max_text_length = 5\n",
    "\n",
    "# Create random image data and labels\n",
    "image_data = np.random.rand(num_samples, img_height, img_width, 1).astype(np.float32)\n",
    "labels = np.random.randint(0, num_classes, size=(num_samples, max_text_length)).astype(np.int32)\n",
    "\n",
    "# Make labels sparse\n",
    "sparse_labels = []\n",
    "for label in labels:\n",
    "    sparse_label = [x for digit in label for x in [digit, 0]]\n",
    "    sparse_label.pop()\n",
    "    sparse_labels.append(sparse_label)\n",
    "sparse_labels = np.array(sparse_labels)\n",
    "\n",
    "# Define the model\n",
    "input_img = layers.Input(shape=(img_height, img_width, 1))\n",
    "conv_1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "maxpool_1 = layers.MaxPooling2D((2, 2))(conv_1)\n",
    "conv_2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(maxpool_1)\n",
    "maxpool_2 = layers.MaxPooling2D((2, 2))(conv_2)\n",
    "\n",
    "# Convert the 2D feature maps to a sequence\n",
    "reshaped = layers.Reshape((-1, 64))(maxpool_2)\n",
    "lstm = layers.LSTM(128, return_sequences=True)(reshaped)\n",
    "output = layers.Dense(num_classes + 1, activation='softmax')(lstm)  # +1 for CTC blank label\n",
    "\n",
    "test_model = Model(inputs=input_img, outputs=output)\n",
    "\n",
    "# CTC loss function\n",
    "def ctc_loss(y_true, y_pred):\n",
    "    # Reshape y_pred to have shape (batch_size, time_steps, num_classes + 1)\n",
    "    input_length = tf.ones(shape=(tf.shape(y_true)[0],), dtype=tf.int32) * max_text_length\n",
    "    label_length = tf.math.count_nonzero(y_true, axis=-1, keepdims=False, dtype=tf.int32)\n",
    "    \n",
    "    loss = tf.nn.ctc_loss(y_true, y_pred, input_length, label_length, logits_time_major=False)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Compile the model with CTC loss\n",
    "test_model.compile(optimizer='adam', loss=ctc_loss, metrics=[accuracy])\n",
    "\n",
    "# Train the model\n",
    "test_model.fit(image_data, labels, epochs=5, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
